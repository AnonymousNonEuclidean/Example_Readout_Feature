{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Example for Paper**: [Non-Euclidean Universal Approximation](https://arxiv.org/abs/2006.02341)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preping\n",
    "\n",
    "We compare three models in this implementation.  Each are feed-forward networks of the same dimensions:\n",
    "- **Good model**: repsects our assumptions\n",
    "- **Bad model**: does not\n",
    "- **Vanilla model**: is a naive feed-forward benchmark\n",
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Alert(s)\n",
    "import smtplib\n",
    "\n",
    "# CV\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# DL: Tensorflow\n",
    "import tensorflow as tf\n",
    "from keras.utils.layer_utils import count_params\n",
    "from tensorflow.python.framework import ops # Custome Tensorflow Functions\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "# DL: Tensorflow - Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import backend as K\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Formatting:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Pre-Processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from scipy.special import expit\n",
    "\n",
    "# Random Forest & Gradient Boosting (Arch. Construction)\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Structuring\n",
    "from pathlib import Path\n",
    "\n",
    "# Visulatization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Writing, Reading, Exporting, and Importing\n",
    "#from sklearn.externals import joblib\n",
    "import pickle\n",
    "\n",
    "# Timing\n",
    "import time\n",
    "\n",
    "# Misc\n",
    "import gc\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import os\n",
    "\n",
    "# Set-Seed\n",
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Externally-Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Training Utility\n",
    "exec(open('TCP_Util.py').read())\n",
    "# Helper Functions Utility\n",
    "exec(open('Optimal_Deep_Feature_and_Readout_Util.py').read())\n",
    "# Extra Utilities\n",
    "exec(open('Grid_Enhanced_Network.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data_path = \"./data/housing_complete.csv\"\n",
    "X = pd.read_csv(data_path)\n",
    "\n",
    "# Parse/Prepare Data\n",
    "X_train, y_train, X_test, y_test= prepare_data(data_path, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check and Make Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path('./outputs/models/').mkdir(parents=True, exist_ok=True)\n",
    "Path('./outputs/models/Vanilla/').mkdir(parents=True, exist_ok=True)\n",
    "Path('./outputs/models/Deep_Features/').mkdir(parents=True, exist_ok=True)\n",
    "Path('./outputs/tables/').mkdir(parents=True, exist_ok=True)\n",
    "Path('./outputs/results/').mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good Model:\n",
    "Build and train the good model:\n",
    "$$\n",
    "\\rho \\circ f\\circ \\phi:\\mathbb{R}^m\\rightarrow \\mathbb{R}^n.\n",
    "$$\n",
    " - $f$ is a shallow feed-forward network with ReLU activation.  \n",
    " - Readout: $\\rho(x) = \\operatorname{Leaky-ReLU}\\bullet (\\exp(\\tilde{A}_n)x+\\tilde{b}_n)\\circ \\dots \\circ \\operatorname{Leaky-ReLU}\\bullet (\\exp(\\tilde{A}_1)x+\\tilde{b}_1)$\n",
    " - Feature Map: $\\phi(x) = \\operatorname{Leaky-ReLU}\\bullet (\\exp(A_n)x+b_n)\\circ \\dots \\circ\\operatorname{Leaky-ReLU}\\bullet (\\exp(A_1)x+b_1)$\n",
    "\n",
    "where $A_i,\\tilde{A}_j$ are square matrices.  \n",
    "\n",
    "\n",
    "The matrices $\\exp(A_i)$, and $\\exp(\\tilde{A}_i)$ are therefore invertible since $\\exp$ maps any square matrix into the associated [General Linear Group](https://en.wikipedia.org/wiki/General_linear_group).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built Model\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Define Predictive Model                                   #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def def_trainable_layers_Nice_Input_Output(height, Depth_Feature_Map, Depth_Readout_Map, learning_rate, input_dim, output_dim):\n",
    "    #----------------------------#\n",
    "    # Maximally Interacting Layer #\n",
    "    #-----------------------------#\n",
    "    # Initialize Inputs\n",
    "    input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    # Deep Feature Map #\n",
    "    #------------------#\n",
    "    for i_feature_depth in range(Depth_Feature_Map):\n",
    "        # First Layer\n",
    "        if i_feature_depth == 0:\n",
    "            deep_feature_map = fullyConnected_Dense_Invertible(input_dim)(input_layer)\n",
    "            deep_feature_map = tf.nn.leaky_relu(deep_feature_map)\n",
    "        else:\n",
    "            deep_feature_map = fullyConnected_Dense_Invertible(input_dim)(deep_feature_map)\n",
    "            deep_feature_map = tf.nn.leaky_relu(deep_feature_map)\n",
    "    \n",
    "    #------------------#\n",
    "    #   Core Layers    #\n",
    "    #------------------#\n",
    "    core_layers = fullyConnected_Dense(height)(deep_feature_map)\n",
    "    # Activation\n",
    "    core_layers = tf.nn.relu(core_layers)\n",
    "    # Affine Layer (Dense Fully Connected)\n",
    "    output_layers = fullyConnected_Dense(output_dim)(core_layers)\n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    #  Readout Layers  #\n",
    "    #------------------#   \n",
    "    for i_depth_readout in range(Depth_Readout_Map):\n",
    "        # First Layer\n",
    "        if i_feature_depth == 0:\n",
    "            output_layers = fullyConnected_Dense_Invertible(output_dim)(output_layers)\n",
    "            output_layers = tf.nn.leaky_relu(output_layers)\n",
    "        else:\n",
    "            output_layers = fullyConnected_Dense_Invertible(output_dim)(output_layers)\n",
    "            output_layers = tf.nn.leaky_relu(output_layers)\n",
    "    \n",
    "    \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, output_layers)\n",
    "    \n",
    "    \n",
    "    #----------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    trainable_layers_model.compile(optimizer=opt, loss=\"mae\", metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return trainable_layers_model\n",
    "\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Build Predictive Model                                    #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def build_and_predict_nice_model(n_folds , n_jobs):\n",
    "\n",
    "    # Deep Feature Network\n",
    "    Nice_Model_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=def_trainable_layers_Nice_Input_Output, verbose=True)\n",
    "    \n",
    "    # Randomized CV\n",
    "    Nice_Model_CVer = RandomizedSearchCV(estimator=Nice_Model_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(CV_folds, random_state=2020, shuffle=True),\n",
    "                                    param_distributions=param_grid_Nice_Nets,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "    \n",
    "    # Fit\n",
    "    Nice_Model_CVer.fit(X_train,y_train)\n",
    "\n",
    "    # Write Predictions\n",
    "    y_hat_train = Nice_Model_CVer.predict(X_train)\n",
    "    y_hat_test = Nice_Model_CVer.predict(X_test)\n",
    "    \n",
    "    # Return Values\n",
    "    return y_hat_train, y_hat_test\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print('Built Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:  3.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:  3.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14447 samples\n",
      "Epoch 1/200\n",
      "14447/14447 [==============================] - 6s 417us/sample - loss: 19.2548 - mse: 8679.8623 - mae: 19.2548 - mape: 1122.9584\n",
      "Epoch 2/200\n",
      "14447/14447 [==============================] - 1s 63us/sample - loss: 2.2234 - mse: 6.4183 - mae: 2.2234 - mape: 110.1488\n",
      "Epoch 3/200\n",
      "14447/14447 [==============================] - 1s 63us/sample - loss: 2.1128 - mse: 5.9538 - mae: 2.1128 - mape: 103.2541\n",
      "Epoch 4/200\n",
      "14447/14447 [==============================] - 1s 65us/sample - loss: 2.0801 - mse: 5.7831 - mae: 2.0801 - mape: 102.1871\n",
      "Epoch 5/200\n",
      "14447/14447 [==============================] - 1s 65us/sample - loss: 2.1647 - mse: 6.1060 - mae: 2.1647 - mape: 106.3886\n",
      "Epoch 6/200\n",
      "14447/14447 [==============================] - 1s 65us/sample - loss: 2.2660 - mse: 6.5661 - mae: 2.2660 - mape: 112.5991\n",
      "Epoch 7/200\n",
      "14447/14447 [==============================] - 1s 65us/sample - loss: 2.3019 - mse: 6.9457 - mae: 2.3019 - mape: 114.8899\n",
      "Epoch 8/200\n",
      "14447/14447 [==============================] - 1s 64us/sample - loss: 2.1068 - mse: 5.8920 - mae: 2.1068 - mape: 102.9215\n",
      "Epoch 9/200\n",
      "14447/14447 [==============================] - 1s 65us/sample - loss: 2.2350 - mse: 6.3890 - mae: 2.2350 - mape: 110.0898\n",
      "Epoch 10/200\n",
      "14447/14447 [==============================] - 1s 66us/sample - loss: 2.1175 - mse: 5.8939 - mae: 2.1175 - mape: 103.4916\n",
      "Epoch 11/200\n",
      "14447/14447 [==============================] - 1s 65us/sample - loss: 2.0859 - mse: 5.7743 - mae: 2.0859 - mape: 101.8744\n",
      "Epoch 12/200\n",
      "14447/14447 [==============================] - 1s 64us/sample - loss: 2.2607 - mse: 6.5269 - mae: 2.2607 - mape: 111.6119\n",
      "Epoch 13/200\n",
      "14447/14447 [==============================] - 1s 64us/sample - loss: 2.2652 - mse: 6.7039 - mae: 2.2652 - mape: 112.0198\n",
      "Epoch 14/200\n",
      "14447/14447 [==============================] - 1s 63us/sample - loss: 2.1329 - mse: 6.1242 - mae: 2.1329 - mape: 103.7818\n",
      "Epoch 15/200\n",
      "14447/14447 [==============================] - 1s 64us/sample - loss: 2.1138 - mse: 5.9070 - mae: 2.1138 - mape: 102.9137\n",
      "Epoch 16/200\n",
      "14447/14447 [==============================] - 1s 65us/sample - loss: 2.0921 - mse: 5.8729 - mae: 2.0922 - mape: 100.9956\n",
      "Epoch 17/200\n",
      "14447/14447 [==============================] - 1s 66us/sample - loss: 2.2963 - mse: 6.7480 - mae: 2.2963 - mape: 113.4160\n",
      "Epoch 18/200\n",
      "14447/14447 [==============================] - 1s 64us/sample - loss: 2.1302 - mse: 5.9713 - mae: 2.1302 - mape: 103.4146\n",
      "Epoch 19/200\n",
      "14447/14447 [==============================] - 1s 64us/sample - loss: 2.2931 - mse: 6.8033 - mae: 2.2931 - mape: 113.4630\n",
      "Epoch 20/200\n",
      "14447/14447 [==============================] - 1s 66us/sample - loss: 2.1267 - mse: 6.1220 - mae: 2.1267 - mape: 103.6936\n",
      "Epoch 21/200\n",
      "14447/14447 [==============================] - 1s 63us/sample - loss: 2.3519 - mse: 7.0384 - mae: 2.3519 - mape: 116.8548\n",
      "Epoch 22/200\n",
      "14447/14447 [==============================] - 1s 64us/sample - loss: 2.0817 - mse: 5.9710 - mae: 2.0817 - mape: 100.4325\n",
      "Epoch 23/200\n",
      "14447/14447 [==============================] - 1s 64us/sample - loss: 2.0265 - mse: 5.7503 - mae: 2.0265 - mape: 96.5446\n",
      "Epoch 24/200\n",
      "14447/14447 [==============================] - 1s 64us/sample - loss: 2.0478 - mse: 5.8112 - mae: 2.0478 - mape: 98.1355\n",
      "Epoch 25/200\n",
      "14447/14447 [==============================] - 1s 64us/sample - loss: 1.9383 - mse: 5.2179 - mae: 1.9383 - mape: 91.1985\n",
      "Epoch 26/200\n",
      "14447/14447 [==============================] - 1s 92us/sample - loss: 1.7778 - mse: 4.6044 - mae: 1.7778 - mape: 81.2337\n",
      "Epoch 27/200\n",
      "14447/14447 [==============================] - 1s 66us/sample - loss: 2.0466 - mse: 5.8770 - mae: 2.0466 - mape: 97.8700\n",
      "Epoch 28/200\n",
      "14447/14447 [==============================] - 1s 64us/sample - loss: 1.8839 - mse: 5.0305 - mae: 1.8839 - mape: 87.2112\n",
      "Epoch 29/200\n",
      "14447/14447 [==============================] - 1s 92us/sample - loss: 1.8214 - mse: 4.8087 - mae: 1.8214 - mape: 84.4639\n",
      "Epoch 30/200\n",
      "14447/14447 [==============================] - 1s 92us/sample - loss: 1.6477 - mse: 4.2590 - mae: 1.6477 - mape: 74.2238\n",
      "Epoch 31/200\n",
      "14447/14447 [==============================] - 1s 90us/sample - loss: 1.6477 - mse: 4.1922 - mae: 1.6477 - mape: 73.9316\n",
      "Epoch 32/200\n",
      "14447/14447 [==============================] - 1s 89us/sample - loss: 1.5487 - mse: 3.8000 - mae: 1.5487 - mape: 68.8277\n",
      "Epoch 33/200\n",
      "14447/14447 [==============================] - 1s 93us/sample - loss: 1.6809 - mse: 4.4182 - mae: 1.6809 - mape: 75.0523\n",
      "Epoch 34/200\n",
      "14447/14447 [==============================] - 1s 82us/sample - loss: 1.5550 - mse: 3.8401 - mae: 1.5550 - mape: 70.5133\n",
      "Epoch 35/200\n",
      "14447/14447 [==============================] - 1s 67us/sample - loss: 1.2120 - mse: 2.6181 - mae: 1.2120 - mape: 53.7907\n",
      "Epoch 36/200\n",
      "14447/14447 [==============================] - 1s 65us/sample - loss: 1.2711 - mse: 2.8226 - mae: 1.2711 - mape: 56.2557\n",
      "Epoch 37/200\n",
      "14447/14447 [==============================] - 1s 65us/sample - loss: 1.4203 - mse: 3.3626 - mae: 1.4203 - mape: 63.2180\n",
      "Epoch 38/200\n",
      "14447/14447 [==============================] - 1s 64us/sample - loss: 1.1476 - mse: 2.3666 - mae: 1.1476 - mape: 51.8413\n",
      "Epoch 39/200\n",
      "14447/14447 [==============================] - 1s 65us/sample - loss: 1.0388 - mse: 2.0062 - mae: 1.0388 - mape: 48.9333\n",
      "Epoch 40/200\n",
      "14447/14447 [==============================] - 1s 66us/sample - loss: 1.0842 - mse: 2.1575 - mae: 1.0842 - mape: 50.3331\n",
      "Epoch 41/200\n",
      "14447/14447 [==============================] - 1s 63us/sample - loss: 1.1245 - mse: 2.3032 - mae: 1.1245 - mape: 51.8187\n",
      "Epoch 42/200\n",
      "14447/14447 [==============================] - 1s 64us/sample - loss: 1.1408 - mse: 2.3475 - mae: 1.1408 - mape: 52.0869\n",
      "Epoch 43/200\n",
      "14447/14447 [==============================] - 1s 64us/sample - loss: 0.9548 - mse: 1.7012 - mae: 0.9548 - mape: 48.0871\n",
      "Epoch 44/200\n",
      "14447/14447 [==============================] - 1s 65us/sample - loss: 1.0003 - mse: 1.8638 - mae: 1.0003 - mape: 48.9341\n",
      "Epoch 45/200\n",
      "14447/14447 [==============================] - 1s 76us/sample - loss: 0.8510 - mse: 1.3465 - mae: 0.8510 - mape: 47.5531\n",
      "Epoch 46/200\n",
      "14447/14447 [==============================] - 1s 78us/sample - loss: 0.8930 - mse: 1.4975 - mae: 0.8930 - mape: 47.5819\n",
      "Epoch 47/200\n",
      "14447/14447 [==============================] - 1s 64us/sample - loss: 0.8958 - mse: 1.5025 - mae: 0.8958 - mape: 47.7774\n",
      "Epoch 48/200\n",
      "14447/14447 [==============================] - 1s 64us/sample - loss: 0.8037 - mse: 1.1947 - mae: 0.8037 - mape: 47.5746\n",
      "Epoch 49/200\n",
      "14447/14447 [==============================] - 1s 72us/sample - loss: 0.7967 - mse: 1.1539 - mae: 0.7967 - mape: 46.6764\n",
      "Epoch 50/200\n",
      "14447/14447 [==============================] - 1s 76us/sample - loss: 0.8316 - mse: 1.2848 - mae: 0.8316 - mape: 47.2596\n",
      "Epoch 51/200\n",
      "14447/14447 [==============================] - 2s 106us/sample - loss: 0.8216 - mse: 1.2331 - mae: 0.8216 - mape: 47.5245\n",
      "Epoch 52/200\n",
      "14447/14447 [==============================] - 1s 91us/sample - loss: 0.8633 - mse: 1.3841 - mae: 0.8633 - mape: 47.5835\n",
      "Epoch 53/200\n",
      "14447/14447 [==============================] - 1s 94us/sample - loss: 0.7844 - mse: 1.0945 - mae: 0.7844 - mape: 47.6883\n",
      "Epoch 54/200\n",
      "14447/14447 [==============================] - 1s 94us/sample - loss: 0.7273 - mse: 0.9123 - mae: 0.7273 - mape: 46.7315\n",
      "Epoch 55/200\n",
      "14447/14447 [==============================] - 1s 91us/sample - loss: 0.7554 - mse: 1.0212 - mae: 0.7554 - mape: 46.4561\n",
      "Epoch 56/200\n",
      "14447/14447 [==============================] - 1s 90us/sample - loss: 0.7787 - mse: 1.0946 - mae: 0.7787 - mape: 46.9790\n",
      "Epoch 57/200\n",
      "14447/14447 [==============================] - 1s 96us/sample - loss: 0.8099 - mse: 1.2232 - mae: 0.8099 - mape: 46.7526\n",
      "Epoch 58/200\n",
      "14447/14447 [==============================] - 1s 76us/sample - loss: 0.7422 - mse: 0.9648 - mae: 0.7422 - mape: 46.6643\n",
      "Epoch 59/200\n",
      "14447/14447 [==============================] - 1s 64us/sample - loss: 0.7347 - mse: 0.9466 - mae: 0.7347 - mape: 46.1540\n",
      "Epoch 60/200\n",
      "14447/14447 [==============================] - 1s 68us/sample - loss: 0.7379 - mse: 0.9536 - mae: 0.7379 - mape: 46.2875\n",
      "Epoch 61/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14447/14447 [==============================] - 1s 67us/sample - loss: 0.7681 - mse: 1.0649 - mae: 0.7681 - mape: 46.4083\n",
      "Epoch 62/200\n",
      "14447/14447 [==============================] - 1s 66us/sample - loss: 0.7660 - mse: 1.0564 - mae: 0.7660 - mape: 46.4183\n",
      "Epoch 63/200\n",
      "14447/14447 [==============================] - 1s 66us/sample - loss: 0.7655 - mse: 1.0408 - mae: 0.7655 - mape: 46.7181\n",
      "Epoch 64/200\n",
      "14447/14447 [==============================] - 1s 65us/sample - loss: 0.7317 - mse: 0.9414 - mae: 0.7317 - mape: 46.0705\n",
      "Epoch 65/200\n",
      "14447/14447 [==============================] - 1s 65us/sample - loss: 0.6997 - mse: 0.8422 - mae: 0.6997 - mape: 45.5321\n",
      "Epoch 66/200\n",
      "14447/14447 [==============================] - 1s 66us/sample - loss: 0.7229 - mse: 0.9085 - mae: 0.7229 - mape: 45.7802\n",
      "Epoch 67/200\n",
      "14447/14447 [==============================] - 1s 65us/sample - loss: 0.7180 - mse: 0.9037 - mae: 0.7180 - mape: 45.5487\n",
      "Epoch 68/200\n",
      "14447/14447 [==============================] - 1s 67us/sample - loss: 0.6834 - mse: 0.8007 - mae: 0.6834 - mape: 44.7504\n",
      "Epoch 69/200\n",
      "14447/14447 [==============================] - 1s 64us/sample - loss: 0.6950 - mse: 0.8432 - mae: 0.6950 - mape: 44.5119\n",
      "Epoch 70/200\n",
      "14447/14447 [==============================] - 1s 65us/sample - loss: 0.7346 - mse: 0.9654 - mae: 0.7346 - mape: 44.9687\n",
      "Epoch 71/200\n",
      "14447/14447 [==============================] - 1s 65us/sample - loss: 0.6715 - mse: 0.7711 - mae: 0.6715 - mape: 44.1434\n",
      "Epoch 72/200\n",
      "14447/14447 [==============================] - 1s 66us/sample - loss: 0.6711 - mse: 0.7690 - mae: 0.6711 - mape: 43.9329\n",
      "Epoch 73/200\n",
      "14447/14447 [==============================] - 1s 65us/sample - loss: 0.6855 - mse: 0.8097 - mae: 0.6855 - mape: 44.1408\n",
      "Epoch 74/200\n",
      "14447/14447 [==============================] - 1s 66us/sample - loss: 0.6692 - mse: 0.7682 - mae: 0.6692 - mape: 43.5615\n",
      "Epoch 75/200\n",
      "14447/14447 [==============================] - 1s 64us/sample - loss: 0.6965 - mse: 0.8468 - mae: 0.6965 - mape: 43.7462\n",
      "Epoch 76/200\n",
      "14447/14447 [==============================] - 1s 66us/sample - loss: 0.7024 - mse: 0.8759 - mae: 0.7024 - mape: 43.8258\n",
      "Epoch 77/200\n",
      "14447/14447 [==============================] - 1s 65us/sample - loss: 0.7026 - mse: 0.8721 - mae: 0.7026 - mape: 43.5947\n",
      "Epoch 78/200\n",
      "14447/14447 [==============================] - 1s 66us/sample - loss: 0.6488 - mse: 0.7154 - mae: 0.6488 - mape: 42.8006\n",
      "Epoch 79/200\n",
      "14447/14447 [==============================] - 1s 66us/sample - loss: 0.6429 - mse: 0.7047 - mae: 0.6429 - mape: 42.2469\n",
      "Epoch 80/200\n",
      "14447/14447 [==============================] - 1s 65us/sample - loss: 0.6763 - mse: 0.8004 - mae: 0.6763 - mape: 42.8205\n",
      "Epoch 81/200\n",
      "14447/14447 [==============================] - 1s 65us/sample - loss: 0.6821 - mse: 0.8189 - mae: 0.6821 - mape: 42.7966\n",
      "Epoch 82/200\n",
      "14447/14447 [==============================] - 1s 64us/sample - loss: 0.6473 - mse: 0.7194 - mae: 0.6473 - mape: 42.1024\n",
      "Epoch 83/200\n",
      "14447/14447 [==============================] - 1s 65us/sample - loss: 0.6645 - mse: 0.7739 - mae: 0.6645 - mape: 42.2913\n",
      "Epoch 84/200\n",
      "14447/14447 [==============================] - 1s 65us/sample - loss: 0.6538 - mse: 0.7527 - mae: 0.6538 - mape: 41.6804\n",
      "Epoch 85/200\n",
      "14447/14447 [==============================] - 1s 66us/sample - loss: 0.6213 - mse: 0.6602 - mae: 0.6213 - mape: 40.8663\n",
      "Epoch 86/200\n",
      "14447/14447 [==============================] - 1s 65us/sample - loss: 0.6287 - mse: 0.6847 - mae: 0.6287 - mape: 40.8681\n",
      "Epoch 87/200\n",
      "14447/14447 [==============================] - 1s 66us/sample - loss: 0.6380 - mse: 0.7037 - mae: 0.6380 - mape: 41.0826\n",
      "Epoch 88/200\n",
      "14447/14447 [==============================] - 1s 64us/sample - loss: 0.6272 - mse: 0.6796 - mae: 0.6272 - mape: 40.6173\n",
      "Epoch 89/200\n",
      "14447/14447 [==============================] - 1s 65us/sample - loss: 0.6257 - mse: 0.6756 - mae: 0.6257 - mape: 40.1978\n",
      "Epoch 90/200\n",
      "14447/14447 [==============================] - 1s 65us/sample - loss: 0.6606 - mse: 0.7719 - mae: 0.6606 - mape: 41.0257\n",
      "Epoch 91/200\n",
      "14447/14447 [==============================] - 1s 64us/sample - loss: 0.6147 - mse: 0.6510 - mae: 0.6147 - mape: 39.8491\n",
      "Epoch 92/200\n",
      "14447/14447 [==============================] - 1s 68us/sample - loss: 0.5948 - mse: 0.6092 - mae: 0.5948 - mape: 38.9708\n",
      "Epoch 93/200\n",
      "14447/14447 [==============================] - 1s 68us/sample - loss: 0.6226 - mse: 0.6705 - mae: 0.6226 - mape: 39.7049\n",
      "Epoch 94/200\n",
      "14447/14447 [==============================] - 1s 64us/sample - loss: 0.5927 - mse: 0.6051 - mae: 0.5927 - mape: 38.6613\n",
      "Epoch 95/200\n",
      "14447/14447 [==============================] - 1s 65us/sample - loss: 0.6062 - mse: 0.6393 - mae: 0.6062 - mape: 38.8397\n",
      "Epoch 96/200\n",
      "14447/14447 [==============================] - 1s 66us/sample - loss: 0.6349 - mse: 0.7195 - mae: 0.6349 - mape: 39.6086\n",
      "Epoch 97/200\n",
      "14447/14447 [==============================] - 1s 65us/sample - loss: 0.6253 - mse: 0.6872 - mae: 0.6253 - mape: 39.1260\n",
      "Epoch 98/200\n",
      "14447/14447 [==============================] - 1s 68us/sample - loss: 0.5949 - mse: 0.6156 - mae: 0.5949 - mape: 38.1132\n",
      "Epoch 99/200\n",
      "14447/14447 [==============================] - 1s 65us/sample - loss: 0.5825 - mse: 0.5816 - mae: 0.5825 - mape: 37.7376\n",
      "Epoch 100/200\n",
      "14447/14447 [==============================] - 1s 66us/sample - loss: 0.6048 - mse: 0.6394 - mae: 0.6048 - mape: 38.1817\n",
      "Epoch 101/200\n",
      "14447/14447 [==============================] - 1s 66us/sample - loss: 0.5985 - mse: 0.6277 - mae: 0.5985 - mape: 37.8352\n",
      "Epoch 102/200\n",
      "14447/14447 [==============================] - 1s 65us/sample - loss: 0.5971 - mse: 0.6246 - mae: 0.5971 - mape: 37.7259\n",
      "Epoch 103/200\n",
      "14447/14447 [==============================] - 1s 65us/sample - loss: 0.6235 - mse: 0.6850 - mae: 0.6235 - mape: 38.4056\n",
      "Epoch 104/200\n",
      "14447/14447 [==============================] - 1s 66us/sample - loss: 0.5828 - mse: 0.5902 - mae: 0.5828 - mape: 37.1723\n",
      "Epoch 105/200\n",
      "14447/14447 [==============================] - 1s 65us/sample - loss: 0.5833 - mse: 0.5960 - mae: 0.5833 - mape: 37.0675\n",
      "Epoch 106/200\n",
      "14447/14447 [==============================] - 1s 66us/sample - loss: 0.5952 - mse: 0.6280 - mae: 0.5952 - mape: 37.3229\n",
      "Epoch 107/200\n",
      "14447/14447 [==============================] - 1s 64us/sample - loss: 0.5824 - mse: 0.5885 - mae: 0.5824 - mape: 36.8260\n",
      "Epoch 108/200\n",
      "14447/14447 [==============================] - 1s 64us/sample - loss: 0.5751 - mse: 0.5831 - mae: 0.5751 - mape: 36.5827\n",
      "Epoch 109/200\n",
      "14447/14447 [==============================] - 1s 65us/sample - loss: 0.5778 - mse: 0.5853 - mae: 0.5778 - mape: 36.4070\n",
      "Epoch 110/200\n",
      "14447/14447 [==============================] - 1s 65us/sample - loss: 0.5903 - mse: 0.6176 - mae: 0.5903 - mape: 36.5514\n",
      "Epoch 111/200\n",
      "14447/14447 [==============================] - 1s 77us/sample - loss: 0.5919 - mse: 0.6176 - mae: 0.5919 - mape: 36.6710\n",
      "Epoch 112/200\n",
      "14447/14447 [==============================] - 1s 77us/sample - loss: 0.5642 - mse: 0.5619 - mae: 0.5642 - mape: 35.3521\n",
      "Epoch 113/200\n",
      "14447/14447 [==============================] - 1s 68us/sample - loss: 0.5855 - mse: 0.6043 - mae: 0.5855 - mape: 36.2784\n",
      "Epoch 114/200\n",
      "14447/14447 [==============================] - 1s 66us/sample - loss: 0.5632 - mse: 0.5675 - mae: 0.5632 - mape: 35.2629\n",
      "Epoch 115/200\n",
      "14447/14447 [==============================] - 1s 72us/sample - loss: 0.5664 - mse: 0.5694 - mae: 0.5664 - mape: 35.2182\n",
      "Epoch 116/200\n",
      "14447/14447 [==============================] - 1s 84us/sample - loss: 0.5767 - mse: 0.5964 - mae: 0.5767 - mape: 35.5367\n",
      "Epoch 117/200\n",
      "14447/14447 [==============================] - 1s 72us/sample - loss: 0.5808 - mse: 0.6034 - mae: 0.5808 - mape: 35.4523\n",
      "Epoch 118/200\n",
      "14447/14447 [==============================] - 1s 71us/sample - loss: 0.5564 - mse: 0.5551 - mae: 0.5564 - mape: 34.4353\n",
      "Epoch 119/200\n",
      "14447/14447 [==============================] - 1s 75us/sample - loss: 0.5467 - mse: 0.5359 - mae: 0.5467 - mape: 34.0141\n",
      "Epoch 120/200\n",
      "14447/14447 [==============================] - 1s 83us/sample - loss: 0.5690 - mse: 0.5771 - mae: 0.5690 - mape: 34.8171\n",
      "Epoch 121/200\n",
      "14447/14447 [==============================] - 1s 86us/sample - loss: 0.5449 - mse: 0.5347 - mae: 0.5449 - mape: 33.6563\n",
      "Epoch 122/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14447/14447 [==============================] - 1s 69us/sample - loss: 0.5784 - mse: 0.6094 - mae: 0.5784 - mape: 34.8291\n",
      "Epoch 123/200\n",
      "14447/14447 [==============================] - 1s 67us/sample - loss: 0.5632 - mse: 0.5669 - mae: 0.5632 - mape: 34.3936\n",
      "Epoch 124/200\n",
      "14447/14447 [==============================] - 1s 66us/sample - loss: 0.5474 - mse: 0.5397 - mae: 0.5474 - mape: 33.4727\n",
      "Epoch 125/200\n",
      "14447/14447 [==============================] - 1s 68us/sample - loss: 0.5643 - mse: 0.5710 - mae: 0.5643 - mape: 34.2850\n",
      "Epoch 126/200\n",
      "14447/14447 [==============================] - 1s 71us/sample - loss: 0.5645 - mse: 0.5779 - mae: 0.5645 - mape: 34.0642\n",
      "Epoch 127/200\n",
      "14447/14447 [==============================] - 1s 72us/sample - loss: 0.5372 - mse: 0.5259 - mae: 0.5372 - mape: 32.8178\n",
      "Epoch 128/200\n",
      "14447/14447 [==============================] - 1s 72us/sample - loss: 0.5494 - mse: 0.5466 - mae: 0.5494 - mape: 33.3081\n",
      "Epoch 129/200\n",
      "14447/14447 [==============================] - 1s 78us/sample - loss: 0.5323 - mse: 0.5182 - mae: 0.5323 - mape: 32.3841\n",
      "Epoch 130/200\n",
      "14447/14447 [==============================] - 1s 65us/sample - loss: 0.5502 - mse: 0.5486 - mae: 0.5502 - mape: 33.0407\n",
      "Epoch 131/200\n",
      "14447/14447 [==============================] - 1s 66us/sample - loss: 0.5444 - mse: 0.5376 - mae: 0.5444 - mape: 32.7895\n",
      "Epoch 132/200\n",
      "14447/14447 [==============================] - 1s 75us/sample - loss: 0.5401 - mse: 0.5347 - mae: 0.5401 - mape: 32.5965\n",
      "Epoch 133/200\n",
      "14447/14447 [==============================] - 1s 68us/sample - loss: 0.5476 - mse: 0.5507 - mae: 0.5476 - mape: 32.6881\n",
      "Epoch 134/200\n",
      "14447/14447 [==============================] - 1s 67us/sample - loss: 0.5400 - mse: 0.5346 - mae: 0.5400 - mape: 32.4390\n",
      "Epoch 135/200\n",
      "14447/14447 [==============================] - 1s 89us/sample - loss: 0.5337 - mse: 0.5235 - mae: 0.5337 - mape: 32.0238\n",
      "Epoch 136/200\n",
      "14447/14447 [==============================] - 1s 85us/sample - loss: 0.5209 - mse: 0.5032 - mae: 0.5209 - mape: 31.2499\n",
      "Epoch 137/200\n",
      "14447/14447 [==============================] - 1s 82us/sample - loss: 0.5277 - mse: 0.5129 - mae: 0.5277 - mape: 31.5906\n",
      "Epoch 138/200\n",
      "14447/14447 [==============================] - 1s 88us/sample - loss: 0.5458 - mse: 0.5516 - mae: 0.5458 - mape: 32.2529\n",
      "Epoch 139/200\n",
      "14447/14447 [==============================] - 1s 95us/sample - loss: 0.5358 - mse: 0.5302 - mae: 0.5358 - mape: 31.7837\n",
      "Epoch 140/200\n",
      "14447/14447 [==============================] - 1s 81us/sample - loss: 0.5351 - mse: 0.5330 - mae: 0.5351 - mape: 31.8003\n",
      "Epoch 141/200\n",
      "14447/14447 [==============================] - 1s 85us/sample - loss: 0.5342 - mse: 0.5297 - mae: 0.5342 - mape: 31.6896\n",
      "Epoch 142/200\n",
      "14447/14447 [==============================] - 2s 117us/sample - loss: 0.5291 - mse: 0.5212 - mae: 0.5291 - mape: 31.3783\n",
      "Epoch 143/200\n",
      "14447/14447 [==============================] - 1s 93us/sample - loss: 0.5371 - mse: 0.5337 - mae: 0.5371 - mape: 31.6018\n",
      "Epoch 144/200\n",
      "14447/14447 [==============================] - 1s 94us/sample - loss: 0.5193 - mse: 0.5035 - mae: 0.5193 - mape: 30.7752\n",
      "Epoch 145/200\n",
      "14447/14447 [==============================] - 1s 82us/sample - loss: 0.5312 - mse: 0.5238 - mae: 0.5312 - mape: 31.2908\n",
      "Epoch 146/200\n",
      "14447/14447 [==============================] - 1s 71us/sample - loss: 0.5229 - mse: 0.5110 - mae: 0.5229 - mape: 30.7815\n",
      "Epoch 147/200\n",
      "14447/14447 [==============================] - 1s 71us/sample - loss: 0.5296 - mse: 0.5232 - mae: 0.5296 - mape: 31.0563\n",
      "Epoch 148/200\n",
      "14447/14447 [==============================] - 1s 68us/sample - loss: 0.5187 - mse: 0.5032 - mae: 0.5187 - mape: 30.4719\n",
      "Epoch 149/200\n",
      "14447/14447 [==============================] - 1s 66us/sample - loss: 0.5277 - mse: 0.5220 - mae: 0.5277 - mape: 31.0191\n",
      "Epoch 150/200\n",
      "14447/14447 [==============================] - 1s 85us/sample - loss: 0.5398 - mse: 0.5480 - mae: 0.5398 - mape: 31.3329\n",
      "Epoch 151/200\n",
      "14447/14447 [==============================] - 2s 119us/sample - loss: 0.5160 - mse: 0.5050 - mae: 0.5160 - mape: 30.3872\n",
      "Epoch 152/200\n",
      "14447/14447 [==============================] - 2s 159us/sample - loss: 0.5414 - mse: 0.5453 - mae: 0.5414 - mape: 31.5662\n",
      "Epoch 153/200\n",
      "14447/14447 [==============================] - 2s 130us/sample - loss: 0.5178 - mse: 0.5076 - mae: 0.5178 - mape: 30.1787\n",
      "Epoch 154/200\n",
      "14447/14447 [==============================] - 2s 104us/sample - loss: 0.5169 - mse: 0.5063 - mae: 0.5169 - mape: 30.1272\n",
      "Epoch 155/200\n",
      "14447/14447 [==============================] - 2s 110us/sample - loss: 0.5295 - mse: 0.5268 - mae: 0.5295 - mape: 30.8298\n",
      "Epoch 156/200\n",
      "14447/14447 [==============================] - 1s 71us/sample - loss: 0.5287 - mse: 0.5249 - mae: 0.5287 - mape: 30.6963\n",
      "Epoch 157/200\n",
      "14447/14447 [==============================] - 1s 76us/sample - loss: 0.5301 - mse: 0.5248 - mae: 0.5301 - mape: 30.9948\n",
      "Epoch 158/200\n",
      "14447/14447 [==============================] - 1s 68us/sample - loss: 0.5193 - mse: 0.5106 - mae: 0.5193 - mape: 30.2710\n",
      "Epoch 159/200\n",
      "14447/14447 [==============================] - 1s 74us/sample - loss: 0.5202 - mse: 0.5078 - mae: 0.5202 - mape: 30.4184\n",
      "Epoch 160/200\n",
      "14447/14447 [==============================] - 1s 67us/sample - loss: 0.5119 - mse: 0.4953 - mae: 0.5119 - mape: 29.8069\n",
      "Epoch 161/200\n",
      "14447/14447 [==============================] - 2s 114us/sample - loss: 0.5142 - mse: 0.5016 - mae: 0.5142 - mape: 29.9429\n",
      "Epoch 162/200\n",
      "14447/14447 [==============================] - 2s 126us/sample - loss: 0.5243 - mse: 0.5209 - mae: 0.5243 - mape: 30.3751\n",
      "Epoch 163/200\n",
      "14447/14447 [==============================] - 1s 87us/sample - loss: 0.5044 - mse: 0.4844 - mae: 0.5044 - mape: 29.3982\n",
      "Epoch 164/200\n",
      "14447/14447 [==============================] - 1s 89us/sample - loss: 0.5100 - mse: 0.4953 - mae: 0.5100 - mape: 29.5743\n",
      "Epoch 165/200\n",
      "14447/14447 [==============================] - 1s 91us/sample - loss: 0.5213 - mse: 0.5142 - mae: 0.5213 - mape: 30.3247\n",
      "Epoch 166/200\n",
      "14447/14447 [==============================] - 2s 126us/sample - loss: 0.5069 - mse: 0.4908 - mae: 0.5069 - mape: 29.2806\n",
      "Epoch 167/200\n",
      "14447/14447 [==============================] - 2s 125us/sample - loss: 0.5168 - mse: 0.5048 - mae: 0.5168 - mape: 29.9121\n",
      "Epoch 168/200\n",
      "14447/14447 [==============================] - 1s 95us/sample - loss: 0.5120 - mse: 0.4974 - mae: 0.5120 - mape: 29.8485\n",
      "Epoch 169/200\n",
      "14447/14447 [==============================] - 1s 101us/sample - loss: 0.5053 - mse: 0.4844 - mae: 0.5053 - mape: 29.2695\n",
      "Epoch 170/200\n",
      "14447/14447 [==============================] - 2s 112us/sample - loss: 0.5153 - mse: 0.5058 - mae: 0.5153 - mape: 29.6469\n",
      "Epoch 171/200\n",
      "14447/14447 [==============================] - 1s 90us/sample - loss: 0.4954 - mse: 0.4729 - mae: 0.4954 - mape: 28.8072\n",
      "Epoch 172/200\n",
      "14447/14447 [==============================] - 1s 99us/sample - loss: 0.5133 - mse: 0.5025 - mae: 0.5133 - mape: 29.5827\n",
      "Epoch 173/200\n",
      " 4096/14447 [=======>......................] - ETA: 1s - loss: 0.5117 - mse: 0.5047 - mae: 0.5117 - mape: 29.4575"
     ]
    }
   ],
   "source": [
    "# Initialize & User Updates\n",
    "#--------------------------#\n",
    "y_hat_train_good, y_hat_test_good = build_and_predict_nice_model(n_folds = 2, n_jobs = 2)\n",
    "print('Cross-Validated: Good Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bad Model:\n",
    "Build and train the *bad* model:\n",
    "$$\n",
    "\\rho \\circ f\\circ \\phi:\\mathbb{R}^m\\rightarrow \\mathbb{R}^n.\n",
    "$$\n",
    " - $f$ is a shallow feed-forward network with ReLU activation.  \n",
    " - Readout: $\\rho(x) = \\operatorname{ReLU}\\bullet (\\exp(\\tilde{A}_n)x+\\tilde{b}_n)\\circ \\dots \\circ \\operatorname{ReLU}\\bullet (\\exp(\\tilde{A}_1)x+\\tilde{b}_1)$\n",
    " - Feature Map: $\\phi(x) = \\operatorname{ReLU}\\bullet (\\exp(A_n)x+b_n)\\circ \\dots \\circ\\operatorname{ReLU}\\bullet (\\exp(A_1)x+b_1)$\n",
    "\n",
    "where $A_i,\\tilde{A}_j$ are square matrices.  The maps $\\rho$ and $\\phi$ are neither injective nor are they surjective since $x\\mapsto \\operatorname{ReLU}(x)$ is neither injective nor surjective as a map from $\\mathbb{R}^k$ to $\\mathbb{R}^k$; where $m=n$.  \n",
    "\n",
    "*Note*:  The key point here is that the input and output maps are forced to be of the same dimension.  Note that, this also violates the minimal bounds derivated in [this paper](https://arxiv.org/abs/1710.11278) for deep ReLU networks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Define Predictive Model                                   #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def def_trainable_layers_Bad_Input_Output(height, Depth_Feature_Map, Depth_Readout_Map, learning_rate, input_dim, output_dim):\n",
    "    #----------------------------#\n",
    "    # Maximally Interacting Layer #\n",
    "    #-----------------------------#\n",
    "    # Initialize Inputs\n",
    "    input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    # Deep Feature Map #\n",
    "    #------------------#\n",
    "    for i_feature_depth in range(Depth_Feature_Map):\n",
    "        # First Layer\n",
    "        if i_feature_depth == 0:\n",
    "            deep_feature_map = fullyConnected_Dense(input_dim)(input_layer)\n",
    "            deep_feature_map = tf.nn.relu(deep_feature_map)\n",
    "        else:\n",
    "            deep_feature_map = fullyConnected_Dense(input_dim)(deep_feature_map)\n",
    "            deep_feature_map = tf.nn.relu(deep_feature_map)\n",
    "    \n",
    "    #------------------#\n",
    "    #   Core Layers    #\n",
    "    #------------------#\n",
    "    core_layers = fullyConnected_Dense(height)(deep_feature_map)\n",
    "    # Activation\n",
    "    core_layers = tf.nn.relu(core_layers)\n",
    "    # Affine Layer (Dense Fully Connected)\n",
    "    output_layers = fullyConnected_Dense(output_dim)(core_layers)\n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    #  Readout Layers  #\n",
    "    #------------------#   \n",
    "    for i_depth_readout in range(Depth_Readout_Map):\n",
    "        # First Layer\n",
    "        if i_feature_depth == 0:\n",
    "            output_layers = fullyConnected_Dense(output_dim)(output_layers)\n",
    "            output_layers = tf.nn.relu(output_layers)\n",
    "        else:\n",
    "            output_layers = fullyConnected_Dense(output_dim)(output_layers)\n",
    "            output_layers = tf.nn.relu(output_layers)\n",
    "    \n",
    "    \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, output_layers)\n",
    "    \n",
    "    \n",
    "    #----------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    trainable_layers_model.compile(optimizer=opt, loss=\"mae\", metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return trainable_layers_model\n",
    "\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Build Predictive Model                                    #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def build_and_predict_bad_model(n_folds , n_jobs):\n",
    "\n",
    "    # Deep Feature Network\n",
    "    Bad_Model_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=def_trainable_layers_Bad_Input_Output, verbose=True)\n",
    "    \n",
    "    # Randomized CV\n",
    "    Bad_Model_CVer = RandomizedSearchCV(estimator=Bad_Model_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(CV_folds, random_state=2020, shuffle=True),\n",
    "                                    param_distributions=param_grid_Nice_Nets,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "    \n",
    "    # Fit\n",
    "    Bad_Model_CVer.fit(X_train,y_train)\n",
    "\n",
    "    # Write Predictions\n",
    "    y_hat_train = Bad_Model_CVer.predict(X_train)\n",
    "    y_hat_test = Bad_Model_CVer.predict(X_test)\n",
    "    \n",
    "    # Return Values\n",
    "    return y_hat_train, y_hat_test\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print('Built Bad Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize & User Updates\n",
    "#--------------------------#\n",
    "y_hat_train_bad, y_hat_test_bad = build_and_predict_bad_model(n_folds = 2, n_jobs = 2)\n",
    "print('Cross-Validated: Vanilla Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Define Predictive Model                                   #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def def_trainable_layers_Vanilla(height, Depth_Feature_Map, Depth_Readout_Map, learning_rate, input_dim, output_dim):\n",
    "    #----------------------------#\n",
    "    # Maximally Interacting Layer #\n",
    "    #-----------------------------#\n",
    "    # Initialize Inputs\n",
    "    input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    #------------------#\n",
    "    #   Core Layers    #\n",
    "    #------------------#\n",
    "    core_layers = fullyConnected_Dense(height)(input_layer)\n",
    "    # Activation\n",
    "    core_layers = tf.nn.relu(core_layers)\n",
    "    # Affine Layer (Dense Fully Connected)\n",
    "    output_layers = fullyConnected_Dense(output_dim)(core_layers)\n",
    "    \n",
    "    \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, output_layers)\n",
    "    \n",
    "    \n",
    "    #----------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    trainable_layers_model.compile(optimizer=opt, loss=\"mae\", metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return trainable_layers_model\n",
    "\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Build Predictive Model                                    #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def build_and_predict_Vanilla_model(n_folds , n_jobs):\n",
    "\n",
    "    # Deep Feature Network\n",
    "    Vanilla_Model_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=def_trainable_layers_Vanilla, verbose=True)\n",
    "    \n",
    "    # Randomized CV\n",
    "    Vanilla_Model_CVer = RandomizedSearchCV(estimator=Vanilla_Model_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(CV_folds, random_state=2020, shuffle=True),\n",
    "                                    param_distributions=param_grid_Nice_Nets,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "    \n",
    "    # Fit\n",
    "    Vanilla_Model_CVer.fit(X_train,y_train)\n",
    "\n",
    "    # Write Predictions\n",
    "    y_hat_train = Vanilla_Model_CVer.predict(X_train)\n",
    "    y_hat_test = Vanilla_Model_CVer.predict(X_test)\n",
    "    \n",
    "    # Return Values\n",
    "    return y_hat_train, y_hat_test\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print('Built Vanilla Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize & User Updates\n",
    "#--------------------------#\n",
    "y_hat_train_Vanilla, y_hat_test_Vanilla = build_and_predict_Vanilla_model(n_folds = 2, n_jobs = 2)\n",
    "print('Cross-Validated: Vanilla Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record Predictions/ Comparisons\n",
    "Generate Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results with Nice Model\n",
    "#------------------------#\n",
    "Train_Good = y_hat_train_good - y_train\n",
    "Test_Good = y_hat_test_good - y_test\n",
    "score_Train_good = np.mean(np.abs(Train_Good))\n",
    "score_Test_good = np.mean(np.abs(Test_Good))\n",
    "score_Train_good_RMSE = np.mean((Train_Good)**2)\n",
    "score_Test_good_RMSE = np.mean((Test_Good)**2)\n",
    "\n",
    "# Results with Bad Model\n",
    "#-----------------------#\n",
    "Train_Bad = y_hat_train_bad - y_train\n",
    "Test_Bad = y_hat_test_bad - y_test\n",
    "score_Train_bad = np.mean(np.abs(Train_Bad))\n",
    "score_Test_bad = np.mean(np.abs(Test_Bad))\n",
    "score_Train_bad_RMSE = np.mean((Train_Bad)**2)\n",
    "score_Test_bad_RMSE = np.mean((Test_Bad)**2)\n",
    "\n",
    "# Results Vanilla #\n",
    "#-----------------#\n",
    "Train_Vanilla = y_hat_train_Vanilla - y_train\n",
    "Test_Vanilla = y_hat_test_Vanilla - y_test\n",
    "score_Train_Vanilla = np.mean(np.abs(Train_Vanilla))\n",
    "score_Test_Vanilla = np.mean(np.abs(Test_Vanilla))\n",
    "score_Train_Vanilla_RMSE = np.mean((Train_Vanilla)**2)\n",
    "score_Test_Vanilla_RMSE = np.mean((Test_Vanilla)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Metrics\n",
    "#----------------------#\n",
    "performance_out = pd.DataFrame({\n",
    "'Good': np.array([np.mean(score_Train_good),score_Train_good_RMSE,np.mean(score_Test_good),score_Test_good_RMSE]),\n",
    "'Bad': np.array([np.mean(score_Train_bad),score_Train_bad_RMSE,np.mean(score_Test_bad),score_Test_bad_RMSE]),\n",
    "'Vanilla': np.array([np.mean(score_Train_Vanilla),score_Train_Vanilla_RMSE,np.mean(score_Test_Vanilla),score_Test_Vanilla_RMSE])\n",
    "},index=['MAE: Train','MAE: Test','RMSE: Train','RMSE: Test'])\n",
    "\n",
    "# Write Results\n",
    "#---------------#\n",
    "# LaTeX\n",
    "performance_out.to_latex('./outputs/results/Performance.txt')\n",
    "# Write to Txt\n",
    "cur_path = os.path.expanduser('./outputs/results/Performance_text.txt')\n",
    "with open(cur_path, \"w\") as f:\n",
    "    f.write(str(performance_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live Readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Et-Voila!')\n",
    "print(performance_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "####  Fin \n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
