{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Example for Paper**: [Non-Euclidean Universal Approximation](https://arxiv.org/abs/2006.02341)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preping\n",
    "\n",
    "We compare three models in this implementation.  Each are feed-forward networks of the same dimensions:\n",
    "- **Good model**: repsects our assumptions\n",
    "- **Bad model**: does not\n",
    "- **Vanilla model**: is a naive feed-forward benchmark\n",
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Alert(s)\n",
    "import smtplib\n",
    "\n",
    "# CV\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# DL: Tensorflow\n",
    "import tensorflow as tf\n",
    "from keras.utils.layer_utils import count_params\n",
    "from tensorflow.python.framework import ops # Custome Tensorflow Functions\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "# DL: Tensorflow - Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import backend as K\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Formatting:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Pre-Processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from scipy.special import expit\n",
    "\n",
    "# Random Forest & Gradient Boosting (Arch. Construction)\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Structuring\n",
    "from pathlib import Path\n",
    "\n",
    "# Visulatization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Writing, Reading, Exporting, and Importing\n",
    "#from sklearn.externals import joblib\n",
    "import pickle\n",
    "\n",
    "# Timing\n",
    "import time\n",
    "\n",
    "# Misc\n",
    "import gc\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import os\n",
    "\n",
    "# Set-Seed\n",
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Externally-Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Training Utility\n",
    "exec(open('TCP_Util.py').read())\n",
    "# Helper Functions Utility\n",
    "exec(open('Optimal_Deep_Feature_and_Readout_Util.py').read())\n",
    "# Extra Utilities\n",
    "exec(open('Grid_Enhanced_Network.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data_path = \"./data/housing_complete.csv\"\n",
    "X = pd.read_csv(data_path)\n",
    "\n",
    "# Parse/Prepare Data\n",
    "X_train, y_train, X_test, y_test= prepare_data(data_path, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check and Make Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path('./outputs/models/').mkdir(parents=True, exist_ok=True)\n",
    "Path('./outputs/models/Vanilla/').mkdir(parents=True, exist_ok=True)\n",
    "Path('./outputs/models/Deep_Features/').mkdir(parents=True, exist_ok=True)\n",
    "Path('./outputs/tables/').mkdir(parents=True, exist_ok=True)\n",
    "Path('./outputs/results/').mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Miscelaneous Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "\n",
    "    y_true.shape = (y_true.shape[0], 1)\n",
    "    y_pred.shape = (y_pred.shape[0], 1)\n",
    "    \n",
    "    # ONLY POSSIBLY COMPUTABLE When denotmiator is well-defined; test it:\n",
    "    y_true = y_true[y_true !=0]\n",
    "    y_pred = y_pred[y_true !=0]\n",
    "\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good Model:\n",
    "Build and train the good model:\n",
    "$$\n",
    "\\rho \\circ f\\circ \\phi:\\mathbb{R}^m\\rightarrow \\mathbb{R}^n.\n",
    "$$\n",
    " - $f$ is a shallow feed-forward network with ReLU activation.  \n",
    " - Readout: $\\rho(x) = \\operatorname{Leaky-ReLU}\\bullet (\\exp(\\tilde{A}_n)x+\\tilde{b}_n)\\circ \\dots \\circ \\operatorname{Leaky-ReLU}\\bullet (\\exp(\\tilde{A}_1)x+\\tilde{b}_1)$\n",
    " - Feature Map: $\\phi(x) = \\operatorname{Leaky-ReLU}\\bullet (\\exp(A_n)x+b_n)\\circ \\dots \\circ\\operatorname{Leaky-ReLU}\\bullet (\\exp(A_1)x+b_1)$\n",
    "\n",
    "where $A_i,\\tilde{A}_j$ are square matrices.  \n",
    "\n",
    "\n",
    "The matrices $\\exp(A_i)$, and $\\exp(\\tilde{A}_i)$ are therefore invertible since $\\exp$ maps any square matrix into the associated [General Linear Group](https://en.wikipedia.org/wiki/General_linear_group).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built Model\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Define Predictive Model                                   #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def def_trainable_layers_Nice_Input_Output(height, Depth_Feature_Map, Depth_Readout_Map, learning_rate, input_dim, output_dim):\n",
    "    #----------------------------#\n",
    "    # Maximally Interacting Layer #\n",
    "    #-----------------------------#\n",
    "    # Initialize Inputs\n",
    "    input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    # Deep Feature Map #\n",
    "    #------------------#\n",
    "    for i_feature_depth in range(Depth_Feature_Map):\n",
    "        # First Layer\n",
    "        if i_feature_depth == 0:\n",
    "            deep_feature_map = fullyConnected_Dense_Invertible(input_dim)(input_layer)\n",
    "            deep_feature_map = tf.nn.leaky_relu(deep_feature_map)\n",
    "        else:\n",
    "            deep_feature_map = fullyConnected_Dense_Invertible(input_dim)(deep_feature_map)\n",
    "            deep_feature_map = tf.nn.leaky_relu(deep_feature_map)\n",
    "    \n",
    "    #------------------#\n",
    "    #   Core Layers    #\n",
    "    #------------------#\n",
    "    core_layers = fullyConnected_Dense(height)(deep_feature_map)\n",
    "    # Activation\n",
    "    core_layers = tf.nn.relu(core_layers)\n",
    "    # Affine Layer (Dense Fully Connected)\n",
    "    output_layers = fullyConnected_Dense(output_dim)(core_layers)\n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    #  Readout Layers  #\n",
    "    #------------------#   \n",
    "    for i_depth_readout in range(Depth_Readout_Map):\n",
    "        # First Layer\n",
    "        if i_feature_depth == 0:\n",
    "            output_layers = fullyConnected_Dense_Invertible(output_dim)(output_layers)\n",
    "            output_layers = tf.nn.leaky_relu(output_layers)\n",
    "        else:\n",
    "            output_layers = fullyConnected_Dense_Invertible(output_dim)(output_layers)\n",
    "            output_layers = tf.nn.leaky_relu(output_layers)\n",
    "    \n",
    "    \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, output_layers)\n",
    "    \n",
    "    \n",
    "    #----------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    trainable_layers_model.compile(optimizer=opt, loss=\"mae\", metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return trainable_layers_model\n",
    "\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Build Predictive Model                                    #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def build_and_predict_nice_model(n_folds , n_jobs):\n",
    "\n",
    "    # Deep Feature Network\n",
    "    Nice_Model_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=def_trainable_layers_Nice_Input_Output, verbose=True)\n",
    "    \n",
    "    # Randomized CV\n",
    "    Nice_Model_CVer = RandomizedSearchCV(estimator=Nice_Model_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(CV_folds, random_state=2020, shuffle=True),\n",
    "                                    param_distributions=param_grid_Nice_Nets,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "    \n",
    "    # Fit\n",
    "    Nice_Model_CVer.fit(X_train,y_train)\n",
    "\n",
    "    # Write Predictions\n",
    "    y_hat_train = Nice_Model_CVer.predict(X_train)\n",
    "    y_hat_test = Nice_Model_CVer.predict(X_test)\n",
    "    \n",
    "    # Return Values\n",
    "    return y_hat_train, y_hat_test\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print('Built Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/kratsioa/.local/lib/python3.7/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 1 is smaller than n_iter=5. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:   10.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:   10.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14447 samples\n",
      "14447/14447 [==============================] - 4s 286us/sample - loss: 1.2581 - mse: 3.9313 - mae: 1.2581 - mape: 65.5548\n",
      "14447/14447 [==============================] - 1s 84us/sample\n",
      "6193/6193 [==============================] - 0s 55us/sample\n",
      "Cross-Validated: Good Model\n"
     ]
    }
   ],
   "source": [
    "# Initialize & User Updates\n",
    "#--------------------------#\n",
    "y_hat_train_good, y_hat_test_good = build_and_predict_nice_model(n_folds = 2, n_jobs = 2)\n",
    "print('Cross-Validated: Good Model')\n",
    "\n",
    "\n",
    "# Results with Nice Model\n",
    "#------------------------#\n",
    "Train_Good = y_hat_train_good - y_train\n",
    "Test_Good = y_hat_test_good - y_test\n",
    "score_Train_good = np.mean(np.abs(Train_Good))\n",
    "score_Test_good = np.mean(np.abs(Test_Good))\n",
    "score_Train_good_RMSE = np.mean((Train_Good)**2)\n",
    "score_Test_good_RMSE = np.mean((Test_Good)**2)\n",
    "score_Train_good_MAPE = mean_absolute_percentage_error(y_train,y_hat_train_good)\n",
    "score_Test_good_MAPE = mean_absolute_percentage_error(y_test,y_hat_test_good)\n",
    "\n",
    "# Performance Metrics\n",
    "#----------------------#\n",
    "performance_out = pd.DataFrame({\n",
    "'Good': np.array([np.mean(score_Train_good),score_Train_good_RMSE,np.mean(score_Test_good),score_Test_good_RMSE,score_Train_good_MAPE,score_Test_good_MAPE])\n",
    "},index=['MAE: Train','MAE: Test','RMSE: Train','RMSE: Test', 'MAPE: Train', 'MAPE: Test'])\n",
    "\n",
    "# Write Results\n",
    "#---------------#\n",
    "# LaTeX\n",
    "performance_out.to_latex('./outputs/results/Performance.txt')\n",
    "# Write to Txt\n",
    "cur_path = os.path.expanduser('./outputs/results/Performance_text.txt')\n",
    "with open(cur_path, \"w\") as f:\n",
    "    f.write(str(performance_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bad Model:\n",
    "Build and train the *bad* model:\n",
    "$$\n",
    "\\rho \\circ f\\circ \\phi:\\mathbb{R}^m\\rightarrow \\mathbb{R}^n.\n",
    "$$\n",
    " - $f$ is a shallow feed-forward network with ReLU activation.  \n",
    " - Readout: $\\rho(x) = \\operatorname{ReLU}\\bullet (\\tilde{A}_nx+\\tilde{b}_n)\\circ \\dots \\circ \\operatorname{ReLU}\\bullet (\\tilde{A}_1x+\\tilde{b}_1)$\n",
    " - Feature Map: $\\phi(x) = \\operatorname{ReLU}\\bullet (A_nx+b_n)\\circ \\dots \\circ\\operatorname{ReLU}\\bullet (A_1x+b_1)$\n",
    "\n",
    "where $A_i,\\tilde{A}_j$ are square matrices.  The maps $\\rho$ and $\\phi$ are neither injective nor are they surjective since $x\\mapsto \\operatorname{ReLU}(x)$ is neither injective nor surjective as a map from $\\mathbb{R}^k$ to $\\mathbb{R}^k$; where $m=n$.  \n",
    "\n",
    "*Note*:  The key point here is that the input and output maps are forced to be of the same dimension.  Note that, this also violates the minimal bounds derivated in [this paper](https://arxiv.org/abs/1710.11278) for deep ReLU networks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built Bad Model\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Define Predictive Model                                   #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def def_trainable_layers_Bad_Input_Output(height, Depth_Feature_Map, Depth_Readout_Map, learning_rate, input_dim, output_dim):\n",
    "    #----------------------------#\n",
    "    # Maximally Interacting Layer #\n",
    "    #-----------------------------#\n",
    "    # Initialize Inputs\n",
    "    input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    # Deep Feature Map #\n",
    "    #------------------#\n",
    "    for i_feature_depth in range(Depth_Feature_Map):\n",
    "        # First Layer\n",
    "        if i_feature_depth == 0:\n",
    "            deep_feature_map = fullyConnected_Dense(input_dim)(input_layer)\n",
    "            deep_feature_map = tf.nn.relu(deep_feature_map)\n",
    "        else:\n",
    "            deep_feature_map = fullyConnected_Dense(input_dim)(deep_feature_map)\n",
    "            deep_feature_map = tf.nn.relu(deep_feature_map)\n",
    "    \n",
    "    #------------------#\n",
    "    #   Core Layers    #\n",
    "    #------------------#\n",
    "    core_layers = fullyConnected_Dense(height)(deep_feature_map)\n",
    "    # Activation\n",
    "    core_layers = tf.nn.relu(core_layers)\n",
    "    # Affine Layer (Dense Fully Connected)\n",
    "    output_layers = fullyConnected_Dense(output_dim)(core_layers)\n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    #  Readout Layers  #\n",
    "    #------------------#   \n",
    "    for i_depth_readout in range(Depth_Readout_Map):\n",
    "        # First Layer\n",
    "        if i_feature_depth == 0:\n",
    "            output_layers = fullyConnected_Dense(output_dim)(output_layers)\n",
    "            output_layers = tf.nn.relu(output_layers)\n",
    "        else:\n",
    "            output_layers = fullyConnected_Dense(output_dim)(output_layers)\n",
    "            output_layers = tf.nn.relu(output_layers)\n",
    "    \n",
    "    \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, output_layers)\n",
    "    \n",
    "    \n",
    "    #----------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    trainable_layers_model.compile(optimizer=opt, loss=\"mae\", metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return trainable_layers_model\n",
    "\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Build Predictive Model                                    #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def build_and_predict_bad_model(n_folds , n_jobs):\n",
    "\n",
    "    # Deep Feature Network\n",
    "    Bad_Model_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=def_trainable_layers_Bad_Input_Output, verbose=True)\n",
    "    \n",
    "    # Randomized CV\n",
    "    Bad_Model_CVer = RandomizedSearchCV(estimator=Bad_Model_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(CV_folds, random_state=2020, shuffle=True),\n",
    "                                    param_distributions=param_grid_Nice_Nets,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "    \n",
    "    # Fit\n",
    "    Bad_Model_CVer.fit(X_train,y_train)\n",
    "\n",
    "    # Write Predictions\n",
    "    y_hat_train = Bad_Model_CVer.predict(X_train)\n",
    "    y_hat_test = Bad_Model_CVer.predict(X_test)\n",
    "    \n",
    "    # Return Values\n",
    "    return y_hat_train, y_hat_test\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print('Built Bad Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/kratsioa/.local/lib/python3.7/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 1 is smaller than n_iter=5. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    3.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    3.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14447 samples\n",
      "14447/14447 [==============================] - 2s 149us/sample - loss: 2.0736 - mse: 5.6372 - mae: 2.0736 - mape: 100.0000\n",
      "14447/14447 [==============================] - 1s 47us/sample\n",
      "6193/6193 [==============================] - 0s 36us/sample\n",
      "Cross-Validated: Vanilla Model\n"
     ]
    }
   ],
   "source": [
    "# Initialize & User Updates\n",
    "#--------------------------#\n",
    "y_hat_train_bad, y_hat_test_bad = build_and_predict_bad_model(n_folds = 2, n_jobs = 2)\n",
    "print('Cross-Validated: Vanilla Model')\n",
    "\n",
    "# Results with Bad Model\n",
    "#-----------------------#\n",
    "Train_Bad = y_hat_train_bad - y_train\n",
    "Test_Bad = y_hat_test_bad - y_test\n",
    "score_Train_bad = np.mean(np.abs(Train_Bad))\n",
    "score_Test_bad = np.mean(np.abs(Test_Bad))\n",
    "score_Train_bad_RMSE = np.mean((Train_Bad)**2)\n",
    "score_Test_bad_RMSE = np.mean((Test_Bad)**2)\n",
    "score_Train_bad_MAPE = mean_absolute_percentage_error(y_train,y_hat_train_bad)\n",
    "score_Test_bad_MAPE = mean_absolute_percentage_error(y_test,y_hat_test_bad)\n",
    "\n",
    "# Performance Metrics\n",
    "#----------------------#\n",
    "performance_out = pd.DataFrame({\n",
    "'Good': np.array([np.mean(score_Train_good),score_Train_good_RMSE,np.mean(score_Test_good),score_Test_good_RMSE,score_Train_good_MAPE,score_Test_good_MAPE]),\n",
    "'Bad': np.array([np.mean(score_Train_bad),score_Train_bad_RMSE,np.mean(score_Test_bad),score_Test_bad_RMSE,score_Train_bad_MAPE,score_Test_bad_MAPE])\n",
    "},index=['MAE: Train','MAE: Test','RMSE: Train','RMSE: Test', 'MAPE: Train', 'MAPE: Test'])\n",
    "\n",
    "# Write Results\n",
    "#---------------#\n",
    "# LaTeX\n",
    "performance_out.to_latex('./outputs/results/Performance.txt')\n",
    "# Write to Txt\n",
    "cur_path = os.path.expanduser('./outputs/results/Performance_text.txt')\n",
    "with open(cur_path, \"w\") as f:\n",
    "    f.write(str(performance_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built Vanilla Model\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Define Predictive Model                                   #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def def_trainable_layers_Vanilla(height, Depth_Feature_Map, Depth_Readout_Map, learning_rate, input_dim, output_dim):\n",
    "    #----------------------------#\n",
    "    # Maximally Interacting Layer #\n",
    "    #-----------------------------#\n",
    "    # Initialize Inputs\n",
    "    input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    #------------------#\n",
    "    #   Core Layers    #\n",
    "    #------------------#\n",
    "    core_layers = fullyConnected_Dense(height)(input_layer)\n",
    "    # Activation\n",
    "    core_layers = tf.nn.relu(core_layers)\n",
    "    # Affine Layer (Dense Fully Connected)\n",
    "    output_layers = fullyConnected_Dense(output_dim)(core_layers)\n",
    "    \n",
    "    \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, output_layers)\n",
    "    \n",
    "    \n",
    "    #----------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    trainable_layers_model.compile(optimizer=opt, loss=\"mae\", metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return trainable_layers_model\n",
    "\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Build Predictive Model                                    #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def build_and_predict_Vanilla_model(n_folds , n_jobs):\n",
    "\n",
    "    # Deep Feature Network\n",
    "    Vanilla_Model_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=def_trainable_layers_Vanilla, verbose=True)\n",
    "    \n",
    "    # Randomized CV\n",
    "    Vanilla_Model_CVer = RandomizedSearchCV(estimator=Vanilla_Model_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(CV_folds, random_state=2020, shuffle=True),\n",
    "                                    param_distributions=param_grid_Nice_Nets,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "    \n",
    "    # Fit\n",
    "    Vanilla_Model_CVer.fit(X_train,y_train)\n",
    "\n",
    "    # Write Predictions\n",
    "    y_hat_train = Vanilla_Model_CVer.predict(X_train)\n",
    "    y_hat_test = Vanilla_Model_CVer.predict(X_test)\n",
    "    \n",
    "    # Return Values\n",
    "    return y_hat_train, y_hat_test\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print('Built Vanilla Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/kratsioa/.local/lib/python3.7/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 1 is smaller than n_iter=5. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    3.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    3.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14447 samples\n",
      "14447/14447 [==============================] - 2s 136us/sample - loss: 1.2048 - mse: 2.8358 - mae: 1.2048 - mape: 59.5882\n",
      "14447/14447 [==============================] - 0s 32us/sample\n",
      "6193/6193 [==============================] - 0s 30us/sample\n",
      "Cross-Validated: Vanilla Model\n"
     ]
    }
   ],
   "source": [
    "# Initialize & User Updates\n",
    "#--------------------------#\n",
    "y_hat_train_Vanilla, y_hat_test_Vanilla = build_and_predict_Vanilla_model(n_folds = 2, n_jobs = 2)\n",
    "print('Cross-Validated: Vanilla Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record Predictions/ Comparisons\n",
    "Generate Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results Vanilla #\n",
    "#-----------------#\n",
    "Train_Vanilla = y_hat_train_Vanilla - y_train\n",
    "Test_Vanilla = y_hat_test_Vanilla - y_test\n",
    "score_Train_Vanilla = np.mean(np.abs(Train_Vanilla))\n",
    "score_Test_Vanilla = np.mean(np.abs(Test_Vanilla))\n",
    "score_Train_Vanilla_RMSE = np.mean((Train_Vanilla)**2)\n",
    "score_Test_Vanilla_RMSE = np.mean((Test_Vanilla)**2)\n",
    "score_Train_Vanilla_MAPE = mean_absolute_percentage_error(y_train,y_hat_train_Vanilla)\n",
    "score_Test_Vanilla_MAPE = mean_absolute_percentage_error(y_test,y_hat_test_Vanilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Metrics\n",
    "#----------------------#\n",
    "performance_out = pd.DataFrame({\n",
    "'Good': np.array([np.mean(score_Train_good),score_Train_good_RMSE,np.mean(score_Test_good),score_Test_good_RMSE,score_Train_good_MAPE,score_Test_good_MAPE]),\n",
    "'Bad': np.array([np.mean(score_Train_bad),score_Train_bad_RMSE,np.mean(score_Test_bad),score_Test_bad_RMSE,score_Train_bad_MAPE,score_Test_bad_MAPE]),\n",
    "'Vanilla': np.array([np.mean(score_Train_Vanilla),score_Train_Vanilla_RMSE,np.mean(score_Test_Vanilla),score_Test_Vanilla_RMSE,score_Train_Vanilla_MAPE,score_Test_Vanilla_MAPE])\n",
    "},index=['MAE: Train','MAE: Test','RMSE: Train','RMSE: Test', 'MAPE: Train', 'MAPE: Test'])\n",
    "\n",
    "# Write Results\n",
    "#---------------#\n",
    "# LaTeX\n",
    "performance_out.to_latex('./outputs/results/Performance.txt')\n",
    "# Write to Txt\n",
    "cur_path = os.path.expanduser('./outputs/results/Performance_text.txt')\n",
    "with open(cur_path, \"w\") as f:\n",
    "    f.write(str(performance_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live Readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Et-Voila!\n",
      "                  Good         Bad    Vanilla\n",
      "MAE: Train    1.120420    2.073648   1.162963\n",
      "MAE: Test     2.858709    5.637205   2.722581\n",
      "RMSE: Train   1.120645    2.056685   1.157742\n",
      "RMSE: Test    2.876520    5.548146   2.802731\n",
      "MAPE: Train  71.296492  100.000000  67.365936\n",
      "MAPE: Test   72.462971  100.000000  68.281516\n"
     ]
    }
   ],
   "source": [
    "print('Et-Voila!')\n",
    "print(performance_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### ðŸ˜Š Fin ðŸ˜Š\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
